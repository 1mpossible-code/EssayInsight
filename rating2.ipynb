{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'ielts_writing_dataset.csv'  # Make sure the path is correct\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the 'Essay' and 'Overall' columns\n",
    "essays = dataset['Essay']\n",
    "scores = dataset['Overall']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "vocab_size = 10000  # Adjust as needed\n",
    "max_length = 300    # Adjust based on your essay length\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "# Tokenizing the essays\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(essays)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Converting essays to sequences\n",
    "sequences = tokenizer.texts_to_sequences(essays)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 64)           640000    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 714369 (2.73 MB)\n",
      "Trainable params: 714369 (2.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Single output node for regression\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.2842 - mean_absolute_error: 0.3441 - val_loss: 0.3320 - val_mean_absolute_error: 0.3447\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 4s 119ms/step - loss: 0.1430 - mean_absolute_error: 0.2593 - val_loss: 0.3566 - val_mean_absolute_error: 0.3936\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.0925 - mean_absolute_error: 0.1989 - val_loss: 0.3499 - val_mean_absolute_error: 0.3629\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 4s 121ms/step - loss: 0.0652 - mean_absolute_error: 0.1532 - val_loss: 0.3804 - val_mean_absolute_error: 0.3872\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 4s 124ms/step - loss: 0.0505 - mean_absolute_error: 0.1233 - val_loss: 0.3768 - val_mean_absolute_error: 0.3808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x290d22790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert scores to numpy array\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_padded, test_padded, train_scores, test_scores = train_test_split(padded, scores, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5  # Adjust as needed\n",
    "model.fit(train_padded, train_scores, epochs=num_epochs, validation_data=(test_padded, test_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 30ms/step - loss: 0.3768 - mean_absolute_error: 0.3808\n",
      "Loss: 0.37680962681770325, Accuracy: 0.3808192312717438\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_padded, test_scores)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/max/projects/EssayInsight/venv/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the rating of essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 436ms/step\n",
      "Essay: Between 1995 and 2010, a study was conducted representing the percentages of people born in Australia, versus people born outside Australia, living in urban, rural, and town. First, in 1995, cities represented the major percentage of habitat by roughly 50 percent, followed by rural areas and towns came in last, among people born in Australia. On the other hand, people born outside Australia, cities showed the most percentages of 6o percent, followed by rural areas and towns. In 2010, among people born in Australia, cities had an increase more than 20 percent increase in the total representation and a major decrease in towns and rural areas. Conversely, people born outside Australia, cities had the most percentage among both studies, followed by rural areas and towns.\n",
      "Predicted Score: [5.4486704]\n",
      "\n",
      "Essay: International sports events require the most well-trained athletes for each country, in order to achieve this goal countries make an effort to build infrastructure designed to train top athletes. Although this policy can indeed make fewer sports facilities for ordinary people, investing in the best athletes is vital to develop competitive sports performances in each country.  On the one hand, building specific infrastructure for the best athletes is crucial in order to get better results at international sports events such as The Olympics or the World Cup. The importance of getting better results is that it creates awareness of the importance of sports in society and motivates more people to do a sport. In this way, investing in these developments can help countries to develop an integral sport policy that can benefit everyone.  On the other hand, one can argue that a negative effect could be that less infrastructure is built for the rest of the people. However, people who practice a sport in their daily life do not necessarily need some facilities to do sports. For example, people often use public spaces to do sports such as running or doing yoga at the nearest park to their home. So, for people who is not top athletes there could be some alternatives for sports facility that ,is not the case for training top athletes. To sum up, I strongly believe countries should invest in specialised infrastructure for their best athletes because in the long term is going to generate more motivation to do sports, to invest in sports at schools and therefore to build more sports infrastructure for everyone.\n",
      "Predicted Score: [8.941037]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_new_essays(new_essays, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Preprocess new essays using the same tokenizer and parameters used for training data.\n",
    "    \"\"\"\n",
    "    # Convert essays to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(new_essays)\n",
    "    # Pad the sequences to ensure uniform length\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    return padded\n",
    "\n",
    "def predict_scores(new_essays, tokenizer, model, max_length):\n",
    "    \"\"\"\n",
    "    Predict scores for new essays.\n",
    "    \"\"\"\n",
    "    # Preprocess essays\n",
    "    preprocessed_essays = preprocess_new_essays(new_essays, tokenizer, max_length)\n",
    "    # Predict scores\n",
    "    predicted_scores = model.predict(preprocessed_essays)\n",
    "    return predicted_scores\n",
    "\n",
    "# Example usage\n",
    "new_essays = [\"Essay text goes here\", \"Essay 2 text goes here\"]\n",
    "predicted_scores = predict_scores(new_essays, tokenizer, model, max_length)\n",
    "\n",
    "# Displaying predicted scores\n",
    "for essay, score in zip(new_essays, predicted_scores):\n",
    "    print(f\"Essay: {essay}\\nPredicted Score: {score}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
